*  í•´ë‹¹ í”„ë¡œì íŠ¸ì—ì„œ ì¤‘ìš”í•˜ë‹¤ê³  ìƒê°í•œ python ì½”ë“œ  <br/>
ğŸ“— test10.py <br/>
ğŸ“— test11.py <br/>
ğŸ“— test12.py <br/>
ğŸ“— test13.py <br/>
----------------------------------------

1) test10.py <br/>

import numpy as np
import pandas as pd
import os
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud

DATA_PATH = '/python/99.project/DATA/' #ë°ì´í„°ê²½ë¡œ ì„¤ì •
print('íŒŒì¼ í¬ê¸°: ')
for file in os.listdir(DATA_PATH):
  if 'txt' in file:
    print(file.ljust(30)+str(round(os.path.getsize(DATA_PATH+ file) / 100000,2))+'MB')
    
#íŠ¸ë ˆì¸ íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸° <br/>
train_data = pd.read_csv(DATA_PATH + 'ratings_train.txt',header = 0, delimiter = '\t', quoting=3)
train_data.head()
print('í•™ìŠµë°ì´í„° ì „ì²´ ê°œìˆ˜: {}'.format(len(train_data)))

train_length = train_data['document'].astype(str).apply(len)
train_length.head()

print('ë¦¬ë·° ê¸¸ì´ ìµœëŒ“ê°’: {}'.format(np.max(train_length)))
print('ë¦¬ë·° ê¸¸ì´ ìµœì†Ÿê°’: {}'.format(np.min(train_length)))
print('ë¦¬ë·° ê¸¸ì´ í‰ê· ê°’: {:.2f}'.format(np.mean(train_length)))
print('ë¦¬ë·° ê¸¸ì´ í‘œì¤€í¸ì°¨: {:.2f}'.format(np.std(train_length)))
print('ë¦¬ë·° ê¸¸ì´ ì¤‘ê°„ê°’: {}'.format(np.median(train_length)))
print('ë¦¬ë·° ê¸¸ì´ ì œ1ì‚¬ë¶„ìœ„: {}'.format(np.percentile(train_length,25)))
print('ë¦¬ë·° ê¸¸ì´ ì œ3ì‚¬ë¶„ìœ„: {}'.format(np.percentile(train_length,75)))

# ë¬¸ìì—´ ì•„ë‹Œ ë°ì´í„° ëª¨ë‘ ì œê±° <br/>
train_review = [review for review in train_data['document'] if type(review) is str]
train_review

wordcloud = WordCloud('HANYGO230.ttf').generate(' '.join(train_review))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

#ê¸ì • 1, ë¶€ì • 0
print('ê¸ì • ë¦¬ë·° ê°¯ìˆ˜: {}'.format(train_data['label'].value_counts()[1]))
print('ë¶€ì • ë¦¬ë·° ê°¯ìˆ˜: {}'.format(train_data['label'].value_counts()[0]))

--------------------------------------------------------------------------------------------------------------------------------
2) test11.py <br/>

import numpy as np
import pandas as pd
import os
import re
import json
from konlpy.tag import Okt
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer

# ë°ì´í„° ê²½ë¡œ ì„¤ì • <br/>
DATA_PATH = '/python/99.project/DATA/'

# íŒŒì¼ ê²½ë¡œê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸ <br/>
file_path = DATA_PATH + 'ratings_train.txt'
if not os.path.exists(file_path):
    raise FileNotFoundError(f"ì§€ì •ëœ ê²½ë¡œì— íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}")

# ë°ì´í„° ë¡œë”© <br/>
train_data = pd.read_csv(file_path, header=0, delimiter='\t', quoting=3)

# ë°ì´í„°ì˜ ì²« ëª‡ í–‰ì„ ì¶œë ¥í•˜ì—¬ í™•ì¸ <br/>
print(train_data.head())

# ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜ <br/>
def preprocessing(review, okt, remove_stopwords=False, stop_words=[]):
    review_text = re.sub('[^ê°€-í£ã„±-ã…ã…-ã…£\\s]', '', review)
    word_review = okt.morphs(review_text, stem=True)
    if remove_stopwords:
        word_review = [token for token in word_review if not token in stop_words]
    return word_review

# ì „ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ <br/>
okt = Okt()
stop_words = ['ì€','ëŠ”','ì´','ê°€','í•˜','ì•„','ê²ƒ','ë“¤','ì˜','ìˆ','ë˜','ìˆ˜','ë³´','ì£¼','ë“±','í•œ']

# ë¦¬ë·° ì¤‘ ì²« ë²ˆì§¸ë¥¼ ì „ì²˜ë¦¬í•´ì„œ í™•ì¸ <br/>
test_review = train_data['document'][0]
if type(test_review) == str:
    processed_review = preprocessing(test_review, okt, remove_stopwords=True, stop_words=stop_words)
    print(f"Original: {test_review}")
    print(f"Processed: {processed_review}")

# ì „ì²´ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ <br/>
clean_train_review = []
for review in train_data['document']:
    if type(review) == str:
        clean_train_review.append(preprocessing(review, okt, remove_stopwords=True, stop_words=stop_words))
    else:
        clean_train_review.append([])

# ì „ì²˜ë¦¬ ê²°ê³¼ì˜ ì²« ëª‡ ê°œë¥¼ ì¶œë ¥í•˜ì—¬ í™•ì¸ <br/>
print(clean_train_review[:4])

#í…ŒìŠ¤íŠ¸ ë¦¬ë·°ë„ ë™ì¼í•˜ê²Œ ì „ì²˜ë¦¬ <br/>
test_data = pd.read_csv(DATA_PATH + 'ratings_test.txt', header = 0, delimiter='\t', quoting=3)

clean_test_review = []
for review in test_data['document']:
  if type(review) == str:
    clean_test_review.append(preprocessing(review, okt, remove_stopwords=True, stop_words=stop_words))
  else:
    clean_test_review.append([])
    
# ì¸ë±ìŠ¤ ë²¡í„° ë³€í™˜ í›„ ì¼ì • ê¸¸ì´ ë„˜ì–´ê°€ê±°ë‚˜ ëª¨ìë¼ëŠ” ë¦¬ë·° íŒ¨ë”©ì²˜ë¦¬ <br/>
tokenizer = Tokenizer()
tokenizer.fit_on_texts(clean_train_review)
train_sequences = tokenizer.texts_to_sequences(clean_train_review)
test_sequences = tokenizer.texts_to_sequences(clean_test_review)

word_vocab = tokenizer.word_index #ë‹¨ì–´ì‚¬ì „í˜•íƒœ
MAX_SEQUENCE_LENGTH = 8 #ë¬¸ì¥ ìµœëŒ€ ê¸¸ì´

#í•™ìŠµ ë°ì´í„° <br/>
train_inputs = pad_sequences(train_sequences,maxlen=MAX_SEQUENCE_LENGTH,padding='post')

#í•™ìŠµ ë°ì´í„° ë¼ë²¨ ë²¡í„°í™” <br/>
train_labels = np.array(train_data['label'])

#í‰ê°€ ë°ì´í„° <br/>
test_inputs = pad_sequences(test_sequences,maxlen=MAX_SEQUENCE_LENGTH,padding='post')
#í‰ê°€ ë°ì´í„° ë¼ë²¨ ë²¡í„°í™” <br/>
test_labels = np.array(test_data['label'])

DEFAULT_PATH  = '/python/99.project/' # ê²½ë¡œì§€ì •
DATA_PATH = 'CLEAN_DATA/' #.npyíŒŒì¼ ì €ì¥ ê²½ë¡œì§€ì •
TRAIN_INPUT_DATA = 'nsmc_train_input.npy'
TRAIN_LABEL_DATA = 'nsmc_train_label.npy'
TEST_INPUT_DATA = 'nsmc_test_input.npy'
TEST_LABEL_DATA = 'nsmc_test_label.npy'
DATA_CONFIGS = 'data_configs.json'

data_configs={}
data_configs['vocab'] = word_vocab
data_configs['vocab_size'] = len(word_vocab) + 1

#ì „ì²˜ë¦¬í•œ ë°ì´í„°ë“¤ íŒŒì¼ë¡œì €ì¥ <br/>
if not os.path.exists(DEFAULT_PATH + DATA_PATH):
  os.makedirs(DEFAULT_PATH+DATA_PATH)

#ì „ì²˜ë¦¬ í•™ìŠµë°ì´í„° ë„˜íŒŒì´ë¡œ ì €ì¥ <br/>
np.save(open(DEFAULT_PATH+DATA_PATH+TRAIN_INPUT_DATA,'wb'),train_inputs)
np.save(open(DEFAULT_PATH+DATA_PATH+TRAIN_LABEL_DATA,'wb'),train_labels) <br/>
#ì „ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ë°ì´í„° ë„˜íŒŒì´ë¡œ ì €ì¥ <br/>
np.save(open(DEFAULT_PATH+DATA_PATH+TEST_INPUT_DATA,'wb'),test_inputs)
np.save(open(DEFAULT_PATH+DATA_PATH+TEST_LABEL_DATA,'wb'),test_labels)
<br/>
#ë°ì´í„° ì‚¬ì „ jsonìœ¼ë¡œ ì €ì¥ <br/>
json.dump(data_configs,open(DEFAULT_PATH + DATA_PATH + DATA_CONFIGS,'w'),ensure_ascii=False)

--------------------------------------------------------------------------------------------------------------------------------
3) test12.py <br/>

import tensorflow as tf
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras import layers
import numpy as np
import pandas as pd
import os
import json
from tqdm import tqdm

# ì „ì²˜ë¦¬ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° <br/>
DATA_PATH = '/python/99.project/CLEAN_DATA/'
DATA_OUT = '/python/99.project/DATA_OUT/'
INPUT_TRAIN_DATA = 'nsmc_train_input.npy'
LABEL_TRAIN_DATA = 'nsmc_train_label.npy'
DATA_CONFIGS = 'data_configs.json'

train_input = np.load(open(os.path.join(DATA_PATH, INPUT_TRAIN_DATA), 'rb'))
train_input = pad_sequences(train_input, maxlen=train_input.shape[1])
train_label = np.load(open(os.path.join(DATA_PATH, LABEL_TRAIN_DATA), 'rb'))
prepro_configs = json.load(open(os.path.join(DATA_PATH, DATA_CONFIGS), 'r'))

model_name = 'cnn_classifier_kr'
BATCH_SIZE = 512
NUM_EPOCHS = 10
VALID_SPLIT = 0.1
MAX_LEN = train_input.shape[1]

kargs = {
    'model_name': model_name,
    'vocab_size': prepro_configs['vocab_size'],
    'embbeding_size': 128,
    'num_filters': 100,
    'dropout_rate': 0.5,
    'hidden_dimension': 250,
    'output_dimension': 1
}

class CNNClassifier(tf.keras.Model):
    def __init__(self, **kargs):
        super(CNNClassifier, self).__init__(name=kargs['model_name'])
        self.embedding = layers.Embedding(input_dim=kargs['vocab_size'], output_dim=kargs['embbeding_size'])
        self.conv_list = [layers.Conv1D(filters=kargs['num_filters'], kernel_size=kernel_size, padding='valid', 
                                        activation=tf.keras.activations.relu,
                                        kernel_constraint=tf.keras.constraints.MaxNorm(max_value=3)) 
                          for kernel_size in [3, 4, 5]]
        self.pooling = layers.GlobalMaxPooling1D()
        self.dropout = layers.Dropout(kargs['dropout_rate'])
        self.fc1 = layers.Dense(units=kargs['hidden_dimension'], activation=tf.keras.activations.relu,
                                kernel_constraint=tf.keras.constraints.MaxNorm(max_value=3.))
        self.fc2 = layers.Dense(units=kargs['output_dimension'], activation=tf.keras.activations.sigmoid,
                                kernel_constraint=tf.keras.constraints.MaxNorm(max_value=3.))

    def call(self, x):
        x = self.embedding(x)
        x = self.dropout(x)
        x = tf.concat([self.pooling(conv(x)) for conv in self.conv_list], axis=1)
        x = self.fc1(x)
        x = self.fc2(x)
        return x

model = CNNClassifier(**kargs)
model.compile(optimizer=tf.keras.optimizers.Adam(),
              loss=tf.keras.losses.BinaryCrossentropy(),
              metrics=[tf.keras.metrics.BinaryAccuracy(name='accuracy')])
<br/>
# ê²€ì¦ ì •í™•ë„ë¥¼ í†µí•œ EarlyStopping ê¸°ëŠ¥ ë° ëª¨ë¸ ì €ì¥ ë°©ì‹ ì§€ì • <br/>
earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=2)
checkpoint_path = os.path.join(DATA_OUT, model_name, 'weights.h5')
checkpoint_dir = os.path.dirname(checkpoint_path)

if not os.path.exists(checkpoint_dir):
    os.makedirs(checkpoint_dir, exist_ok=True)
    print(f"{checkpoint_dir} -- Folder created\n")
else:
    print(f"{checkpoint_dir} -- Folder already exists\n")

cp_callback = ModelCheckpoint(
    checkpoint_path, monitor='val_accuracy', verbose=1, save_best_only=True,
    save_weights_only=True
)

history = model.fit(train_input, train_label, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS,
                    validation_split=VALID_SPLIT, callbacks=[earlystop_callback, cp_callback])
<br/>
# ëª¨ë¸ ì €ì¥í•˜ê¸° <br/>
model_save_path = os.path.join('/python/99.project/MODEL', 'model')
if not os.path.exists(os.path.dirname(model_save_path)):
    os.makedirs(os.path.dirname(model_save_path), exist_ok=True)
model.save(model_save_path)
<br/>
# í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ë° í‰ê°€ <br/>
INPUT_TEST_DATA = 'nsmc_test_input.npy'
LABEL_TEST_DATA = 'nsmc_test_label.npy'
SAVE_FILE_NM = 'weights.h5'

test_input = np.load(open(os.path.join(DATA_PATH, INPUT_TEST_DATA), 'rb'))
test_input = pad_sequences(test_input, maxlen=test_input.shape[1])
test_label_data = np.load(open(os.path.join(DATA_PATH, LABEL_TEST_DATA), 'rb'))

model.load_weights(checkpoint_path)
model.evaluate(test_input, test_label_data)


--------------------------------------------------------------------------------------------------------------------------------
4) test13.py <br/>

import numpy as np
import pandas as pd
import re
import json
from konlpy.tag import Okt
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
import tensorflow as tf
from tensorflow.keras import layers
<br/>
# ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜ <br/>
class CNNClassifier(tf.keras.Model):
    def __init__(self, **kargs):
        super(CNNClassifier, self).__init__(name=kargs['model_name'])
        self.embedding = layers.Embedding(input_dim=kargs['vocab_size'], output_dim=kargs['embbeding_size'])
        self.conv_list = [layers.Conv1D(filters=kargs['num_filters'], kernel_size=kernel_size, padding='valid', 
                                        activation=tf.keras.activations.relu,
                                        kernel_constraint=tf.keras.constraints.MaxNorm(max_value=3)) 
                          for kernel_size in [3, 4, 5]]
        self.pooling = layers.GlobalMaxPooling1D()
        self.dropout = layers.Dropout(kargs['dropout_rate'])
        self.fc1 = layers.Dense(units=kargs['hidden_dimension'], activation=tf.keras.activations.relu,
                                kernel_constraint=tf.keras.constraints.MaxNorm(max_value=3.))
        self.fc2 = layers.Dense(units=kargs['output_dimension'], activation=tf.keras.activations.sigmoid,
                                kernel_constraint=tf.keras.constraints.MaxNorm(max_value=3.))

    def call(self, x):
        x = self.embedding(x)
        x = self.dropout(x)
        x = tf.concat([self.pooling(conv(x)) for conv in self.conv_list], axis=1)
        x = self.fc1(x)
        x = self.fc2(x)
        return x
<br/>
# Oktì™€ Tokenizer ì´ˆê¸°í™” <br/>
okt = Okt()
tokenizer = Tokenizer()
<br/>
# ë°ì´í„° ì„¤ì • ë¶ˆëŸ¬ì˜¤ê¸° <br/>
DATA_CONFIGS = 'data_configs.json'
prepro_configs = json.load(open('/python/99.project/CLEAN_DATA/' + DATA_CONFIGS, 'r'))
word_vocab = prepro_configs['vocab']
<br/>
# Tokenizerì— ë‹¨ì–´ ì‚¬ì „ ì ìš© <br/>
tokenizer.fit_on_texts(list(word_vocab.keys()))
<br/>
# ëª¨ë¸ ì¸ì ì„¤ì • <br/>
kargs = {
    'model_name': 'cnn_classifier_kr',
    'vocab_size': len(word_vocab) + 1,  # vocab_sizeë¥¼ ë‹¨ì–´ ì‚¬ì „ ê¸¸ì´ë¡œ ì„¤ì •
    'embbeding_size': 128,
    'num_filters': 100,
    'dropout_rate': 0.5,
    'hidden_dimension': 250,
    'output_dimension': 1
}
<br/>
# ëª¨ë¸ ì´ˆê¸°í™” ë° ì»´íŒŒì¼ <br/>
model = CNNClassifier(**kargs)
model.compile(optimizer=tf.keras.optimizers.Adam(),
              loss=tf.keras.losses.BinaryCrossentropy(),
              metrics=[tf.keras.metrics.BinaryAccuracy(name='accuracy')])
<br/>
# dummy inputìœ¼ë¡œ ëª¨ë¸ í˜¸ì¶œ <br/>
dummy_input = np.zeros((1, 8), dtype=np.int32)
model(dummy_input)
<br/>
# ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ <br/>
model.load_weights('/python/99.project/DATA_OUT/cnn_classifier_kr/weights.h5')
<br/>
# ë¬¸ì¥ ë¶„ì„ í•¨ìˆ˜ <br/>
def analyze_sentence(sentence):
    MAX_LENGTH = 8  # ë¬¸ì¥ ìµœëŒ€ ê¸¸ì´
    stopwords = ['ì€', 'ëŠ”', 'ì´', 'ê°€', 'í•˜', 'ì•„', 'ê²ƒ', 'ë“¤', 'ì˜', 'ìˆ', 'ë˜', 'ìˆ˜', 'ë³´', 'ì£¼', 'ë“±', 'í•œ']  # ë¶ˆìš©ì–´
<br/>
    # ë¬¸ì¥ ì „ì²˜ë¦¬ <br/>
    sentence = re.sub(r'[^ã„±-ã…ã…-ã…£ê°€-í£\s]', '', sentence)
    sentence = okt.morphs(sentence, stem=True)
    sentence = [word for word in sentence if not word in stopwords]
<br/>
    # ë¬¸ì¥ì„ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜ ë° íŒ¨ë”© <br/>
    vector = tokenizer.texts_to_sequences([sentence])
    pad_new = pad_sequences(vector, maxlen=MAX_LENGTH)
<br/>
    # ì˜ˆì¸¡ ìˆ˜í–‰ <br/>
    predictions = model.predict(pad_new)
    predictions = float(predictions.squeeze())
<br/>
    # ê²°ê³¼ ì¶œë ¥ <br/>
    if predictions > 0.5:
        print("{:.2f}% í™•ë¥ ë¡œ ê¸ì • ë¦¬ë·°ì…ë‹ˆë‹¤.\n".format(predictions * 100))
    else:
        print("{:.2f}% í™•ë¥ ë¡œ ë¶€ì • ë¦¬ë·°ì…ë‹ˆë‹¤.\n".format((1 - predictions) * 100))
<br/>
# ì‚¬ìš©ì ì…ë ¥ ë°›ì•„ì„œ ê°ì„± ë¶„ì„ <br/>
sentence = input('ê°ì„±ë¶„ì„í•  ë¬¸ì¥ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”: ')
analyze_sentence(sentence)
<br/>
